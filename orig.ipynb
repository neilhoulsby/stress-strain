{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d6b419-0051-496b-a218-00150dbe39fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup\n",
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade git+https://github.com/google/flax.git\n",
    "# !pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74cb9bde-2015-4568-ab61-17980d6fa4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#@title Imports { form-width: \"10%\" }\n",
    "\n",
    "from typing import Callable, Any, Optional\n",
    "import flax\n",
    "import flax.training.common_utils\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import pdb\n",
    "\n",
    "from absl import logging\n",
    "from flax.training import train_state\n",
    "from jax import lax\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "print(jax.devices())\n",
    "print(jax.default_backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84fea4de-56c9-4961-8ab0-b5cfd22b2908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1001, 2)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data.npz'\n",
    "\n",
    "with open(DATA_PATH, 'rb') as f:\n",
    "  data = np.load(f)\n",
    "  RAW_DATA = data[\"arr_0\"]\n",
    "\n",
    "# D_RAW_DATA = RAW_DATA[:,1:,:] - RAW_DATA[:,:-1,:]\n",
    "# D_RAW_DATA = np.pad(D_RAW_DATA, [(0,0), (1,0), (0,0)])\n",
    "\n",
    "print(RAW_DATA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b1237f59-7077-4f8e-9b42-ba468d63b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1024\n",
    "TRAIN_SIZE = 200 - 64\n",
    "PAD_VALUE = -1e10\n",
    "is_pad = lambda x : np.isclose(x, PAD_VALUE)\n",
    "\n",
    "def get_datasets_np(data=RAW_DATA, max_len=MAX_LEN, train_size=TRAIN_SIZE):\n",
    "  b, l, _ = data.shape\n",
    "  assert max_len >= l\n",
    "  data = np.pad(data, ([0,0], [0, max_len-l], [0,0]))\n",
    "\n",
    "  return {\n",
    "      'x': data[:TRAIN_SIZE, :MAX_LEN, 0][...,None],\n",
    "      'y': data[:TRAIN_SIZE, :MAX_LEN, 1][...,None],\n",
    "  }, {\n",
    "      'x': data[TRAIN_SIZE:, :MAX_LEN, 0][...,None],\n",
    "      'y': data[TRAIN_SIZE:, :MAX_LEN, 1][...,None],\n",
    "  }\n",
    "\n",
    "def get_datasets_tf(batch_size):\n",
    "  train_np, test_np = get_datasets_np()\n",
    "  train_ds = tf.data.Dataset.from_tensor_slices(train_np)\n",
    "  train_ds = train_ds.cache()\n",
    "  train_ds = train_ds.repeat(None)\n",
    "  train_ds = train_ds.batch(batch_size)\n",
    "  train_ds = train_ds.prefetch(4)\n",
    "  eval_ds = tf.data.Dataset.from_tensor_slices(test_np)\n",
    "  eval_ds = eval_ds.cache()\n",
    "  eval_ds = eval_ds.repeat(1)\n",
    "  eval_ds = eval_ds.batch(batch_size)\n",
    "  eval_ds = eval_ds.prefetch(4)\n",
    "  return train_ds, eval_ds\n",
    "\n",
    "\n",
    "def create_learning_rate_scheduler(\n",
    "    factors='constant * linear_warmup * rsqrt_decay',\n",
    "    base_learning_rate=0.5,\n",
    "    warmup_steps=8000,\n",
    "    decay_factor=0.5,\n",
    "    steps_per_decay=20000,\n",
    "    steps_per_cycle=100000):\n",
    "  \"\"\"creates learning rate schedule.\n",
    "  Interprets factors in the factors string which can consist of:\n",
    "  * constant: interpreted as the constant value,\n",
    "  * linear_warmup: interpreted as linear warmup until warmup_steps,\n",
    "  * rsqrt_decay: divide by square root of max(step, warmup_steps)\n",
    "  * decay_every: Every k steps decay the learning rate by decay_factor.\n",
    "  * cosine_decay: Cyclic cosine decay, uses steps_per_cycle parameter.\n",
    "  Args:\n",
    "    factors: a string with factors separated by '*' that defines the schedule.\n",
    "    base_learning_rate: float, the starting constant for the lr schedule.\n",
    "    warmup_steps: how many steps to warm up for in the warmup schedule.\n",
    "    decay_factor: The amount to decay the learning rate by.\n",
    "    steps_per_decay: How often to decay the learning rate.\n",
    "    steps_per_cycle: Steps per cycle when using cosine decay.\n",
    "  Returns:\n",
    "    a function learning_rate(step): float -> {'learning_rate': float}, the\n",
    "    step-dependent lr.\n",
    "  \"\"\"\n",
    "  factors = [n.strip() for n in factors.split('*')]\n",
    "\n",
    "  def step_fn(step):\n",
    "    \"\"\"Step to learning rate functio.\"\"\"\n",
    "    ret = 1.0\n",
    "    for name in factors:\n",
    "      if name == 'constant':\n",
    "        ret *= base_learning_rate\n",
    "      elif name == 'linear_warmup':\n",
    "        ret *= jnp.minimum(1.0, step / warmup_steps)\n",
    "      elif name == 'rsqrt_decay':\n",
    "        ret /= jnp.sqrt(jnp.maximum(step, warmup_steps))\n",
    "      elif name == 'rsqrt_normalized_decay':\n",
    "        ret *= jnp.sqrt(warmup_steps)\n",
    "        ret /= jnp.sqrt(jnp.maximum(step, warmup_steps))\n",
    "      elif name == 'decay_every':\n",
    "        ret *= (decay_factor**(step // steps_per_decay))\n",
    "      elif name == 'cosine_decay':\n",
    "        progress = jnp.maximum(0.0,\n",
    "                               (step - warmup_steps) / float(steps_per_cycle))\n",
    "        ret *= jnp.maximum(0.0,\n",
    "                           0.5 * (1.0 + jnp.cos(jnp.pi * (progress % 1.0))))\n",
    "      else:\n",
    "        raise ValueError('Unknown factor %s.' % name)\n",
    "    return ret\n",
    "\n",
    "  return step_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e35c695c-361f-4257-9f84-06b1ed917685",
   "metadata": {},
   "outputs": [],
   "source": [
    "@flax.struct.dataclass\n",
    "class TransformerConfig:\n",
    "  \"\"\"Global hyperparameters used to minimize kwarg plumbing.\"\"\"\n",
    "  output_size: int = 1\n",
    "  max_len: int = MAX_LEN\n",
    "  num_layers: int = 2\n",
    "  hidden_dim: int = 16\n",
    "  mlp_dim: int = 64\n",
    "  num_heads: int = 4\n",
    "  dropout_rate: float = 0.0\n",
    "  attention_dropout_rate: float = 0.0\n",
    "  deterministic: bool = False\n",
    "  decode: bool = False\n",
    "  causal_x: bool = True\n",
    "  physics_decoder: bool = False\n",
    "  kernel_init: Callable = nn.initializers.xavier_uniform()\n",
    "  bias_init: Callable = nn.initializers.normal(stddev=1e-6)\n",
    "  posemb_init: Callable = nn.initializers.normal(stddev=0.02)\n",
    "\n",
    "\n",
    "def shift_right(x, axis=1):\n",
    "  \"\"\"Shift the input to the right by padding and slicing on axis.\"\"\"\n",
    "  pad_widths = [(0, 0)] * len(x.shape)\n",
    "  pad_widths[axis] = (1, 0)\n",
    "  padded = jnp.pad(\n",
    "      x, pad_widths, mode='constant', constant_values=x.dtype.type(0))\n",
    "  return lax.dynamic_slice_in_dim(padded, 0, padded.shape[axis] - 1, axis)\n",
    "\n",
    "\n",
    "\n",
    "class AddPositionEmbs(nn.Module):\n",
    "  \"\"\"Adds (optionally learned) positional embeddings to the inputs.\n",
    "\n",
    "  Attributes:\n",
    "    config: TransformerConfig dataclass containing hyperparameters.\n",
    "  \"\"\"\n",
    "  config: TransformerConfig\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    \"\"\"Applies AddPositionEmbs module.\n",
    "\n",
    "    By default this layer uses a fixed sinusoidal embedding table. If a\n",
    "    learned position embedding is desired, pass an initializer to\n",
    "    posemb_init in the configuration.\n",
    "\n",
    "    Args:\n",
    "      inputs: input data.\n",
    "    Returns:\n",
    "      output: `(bs, timesteps, in_dim)`\n",
    "    \"\"\"\n",
    "    cfg = self.config\n",
    "    # inputs.shape is (batch_size, seq_len, hidden_dim)\n",
    "    assert inputs.ndim == 3, ('Number of dimensions should be 3,'\n",
    "                              ' but it is: %d' % inputs.ndim)\n",
    "    length = inputs.shape[1]\n",
    "    pos_emb_shape = (1, cfg.max_len, inputs.shape[-1])\n",
    "    pos_embedding = self.param('pos_embedding',\n",
    "                               nn.initializers.normal(stddev=0.02),\n",
    "                               pos_emb_shape)\n",
    "    return inputs + pos_embedding[:, :length, :]\n",
    "\n",
    "\n",
    "class MlpBlock(nn.Module):\n",
    "  \"\"\"Transformer MLP / feed-forward block.\n",
    "  Attributes:\n",
    "    config: TransformerConfig dataclass containing hyperparameters.\n",
    "    out_dim: optionally specify out dimension.\n",
    "  \"\"\"\n",
    "  config: TransformerConfig\n",
    "  out_dim: Optional[int] = None\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs, deterministic=True):\n",
    "    \"\"\"Applies Transformer MlpBlock module.\"\"\"\n",
    "    cfg = self.config\n",
    "    actual_out_dim = (inputs.shape[-1] if self.out_dim is None\n",
    "                      else self.out_dim)\n",
    "    x = nn.Dense(cfg.mlp_dim,\n",
    "                   kernel_init=cfg.kernel_init,\n",
    "                   bias_init=cfg.bias_init)(inputs)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dropout(rate=cfg.dropout_rate)(x, deterministic=deterministic)\n",
    "    output = nn.Dense(actual_out_dim,\n",
    "                         kernel_init=cfg.kernel_init,\n",
    "                         bias_init=cfg.bias_init)(x)\n",
    "    output = nn.Dropout(rate=cfg.dropout_rate)(\n",
    "        output, deterministic=deterministic)\n",
    "    return output\n",
    "\n",
    "\n",
    "class EncoderDecoder1DBlock(nn.Module):\n",
    "  \"\"\"Transformer encoder-decoder layer.\n",
    "\n",
    "  Args:\n",
    "    config: TransformerConfig dataclass containing hyperparameters.\n",
    "  \"\"\"\n",
    "  config: TransformerConfig\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self,\n",
    "               inputs,\n",
    "               deterministic,\n",
    "               decoder_mask=None):\n",
    "    \"\"\"Applies EncoderDecoder1DBlock module.\n",
    "\n",
    "    Args:\n",
    "      inputs: input data for decoder\n",
    "      decoder_mask: decoder self-attention mask.\n",
    "\n",
    "    Returns:\n",
    "      output after transformer encoder-decoder block.\n",
    "    \"\"\"\n",
    "    cfg = self.config\n",
    "\n",
    "    # Decoder block.\n",
    "    assert inputs.ndim == 3, ('Number of dimensions should be 3,'\n",
    "                              ' but it is: %d' % inputs.ndim)\n",
    "    x = nn.LayerNorm()(inputs)\n",
    "    x = nn.SelfAttention(\n",
    "        num_heads=cfg.num_heads,\n",
    "        qkv_features=cfg.hidden_dim,\n",
    "        kernel_init=cfg.kernel_init,\n",
    "        bias_init=cfg.bias_init,\n",
    "        use_bias=False,\n",
    "        broadcast_dropout=False,\n",
    "        dropout_rate=cfg.attention_dropout_rate,\n",
    "        deterministic=deterministic)(x, mask=decoder_mask)\n",
    "    x = nn.Dropout(rate=cfg.dropout_rate)(\n",
    "        x, deterministic=deterministic)\n",
    "    x = x + inputs\n",
    "\n",
    "    # MLP block.\n",
    "    z = nn.LayerNorm()(x)\n",
    "    z = MlpBlock(config=cfg)(z)\n",
    "\n",
    "    return x + z\n",
    "\n",
    "\n",
    "class PhysicsDecoder(nn.Module):\n",
    "  \"\"\"Transformer Model Decoder for sequence to sequence translation.\n",
    "\n",
    "  Args:\n",
    "    config: TransformerConfig dataclass containing hyperparameters.\n",
    "  \"\"\"\n",
    "  config: TransformerConfig\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self,\n",
    "               inputs,\n",
    "               deterministic,\n",
    "               decoder_mask=None):\n",
    "    \"\"\"Applies Transformer model on the inputs.\n",
    "\n",
    "    Args:\n",
    "      encoded: encoded input data from encoder.\n",
    "      inputs: input data.\n",
    "      decoder_mask: decoder self-attention mask.\n",
    "\n",
    "    Returns:\n",
    "      output of a transformer decoder.\n",
    "    \"\"\"\n",
    "    x = inputs['x']\n",
    "    dx = build_deltas(x)\n",
    "\n",
    "    cfg = self.config\n",
    "    assert cfg.alpha == 0.0\n",
    "\n",
    "    x = nn.Dense(\n",
    "        cfg.hidden_dim // 2,\n",
    "        kernel_init=cfg.kernel_init,\n",
    "        bias_init=cfg.bias_init,\n",
    "        name='embed_x')(x)\n",
    "    dx = nn.Dense(\n",
    "        cfg.hidden_dim // 2,\n",
    "        kernel_init=cfg.kernel_init,\n",
    "        bias_init=cfg.bias_init,\n",
    "        name='embed_dx')(dx)\n",
    "    x = jnp.concatenate([x, dx], axis=-1)\n",
    "    assert x.shape[-1] == cfg.hidden_dim, f\"{x.shape[-1]} != {cfg.hidden_dim}\"\n",
    "\n",
    "    x = nn.Dropout(rate=cfg.dropout_rate)(x, deterministic=deterministic)\n",
    "    x = AddPositionEmbs(config=cfg, name='posembed_output')(x)\n",
    "    x = nn.Dropout(rate=cfg.dropout_rate)(x, deterministic=deterministic)\n",
    "\n",
    "    # Target-Input Decoder\n",
    "    for lyr in range(cfg.num_layers):\n",
    "      x = EncoderDecoder1DBlock(\n",
    "          config=cfg, name=f'encoderdecoderblock_{lyr}')(\n",
    "              x,\n",
    "              deterministic=deterministic,\n",
    "              decoder_mask=decoder_mask)\n",
    "    alphas = nn.LayerNorm(name='alphas')(x)\n",
    "\n",
    "    dfdx = jax.jacfwd(self.f, argnums=0)\n",
    "    dfdalpha = jax.jacfwd(self.f, argnums=1)\n",
    "\n",
    "    x = dfdx(inputs['x'], alphas, cfg)\n",
    "    x = jnp.einsum('abcdbf->abf', x)\n",
    "    dfda = dfdalpha(inputs['x'], alphas, cfg)  # [B,S,C]\n",
    "    dfda = jnp.einsum('abcdbf->abf', dfda)\n",
    "\n",
    "    delta_a = alphas[:,1:,:] - alphas[:,:-1,:]  # [B,S-1,C]\n",
    "    first_delta_a = jnp.zeros([delta_a.shape[0], 1, delta_a.shape[-1]])\n",
    "    delta_a = jnp.concatenate([first_delta_a, delta_a], axis=1)  # [B,S-1,C]\n",
    "    aux = - jnp.sum(dfda * delta_a, axis=-1)  # [B,S]\n",
    "\n",
    "    # prediction (y/sigma), aux value > 0\n",
    "    return x, jnp.zeros_like(x), aux\n",
    "\n",
    "  def f(self, x, alphas, cfg):\n",
    "    \"\"\"alphas [B,S,C], x [X,S,1]\"\"\"\n",
    "    x = jnp.concatenate([alphas, x], axis=-1)\n",
    "    for _ in range(2):\n",
    "      x = nn.Dense(\n",
    "          16,\n",
    "          kernel_init=cfg.kernel_init,\n",
    "          bias_init=cfg.bias_init)(x)\n",
    "      x = nn.LayerNorm()(x)\n",
    "    x = nn.Dense(\n",
    "          1,\n",
    "          kernel_init=cfg.kernel_init,\n",
    "          bias_init=cfg.bias_init)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  \"\"\"Transformer Model Decoder for sequence to sequence translation.\n",
    "\n",
    "  Args:\n",
    "    config: TransformerConfig dataclass containing hyperparameters.\n",
    "  \"\"\"\n",
    "  config: TransformerConfig\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self,\n",
    "               inputs,\n",
    "               deterministic,\n",
    "               decoder_mask=None):\n",
    "    \"\"\"Applies Transformer model on the inputs.\n",
    "\n",
    "    Args:\n",
    "      encoded: encoded input data from encoder.\n",
    "      inputs: input data.\n",
    "      decoder_mask: decoder self-attention mask.\n",
    "\n",
    "    Returns:\n",
    "      output of a transformer decoder.\n",
    "    \"\"\"\n",
    "    x = inputs['x']\n",
    "    dx = build_deltas(x)\n",
    "    # y = inputs['y']\n",
    "\n",
    "    cfg = self.config\n",
    "    # if not cfg.decode:\n",
    "      # y = shift_right(y)\n",
    "\n",
    "    x = nn.Dense(\n",
    "        cfg.hidden_dim // 2,\n",
    "        kernel_init=cfg.kernel_init,\n",
    "        bias_init=cfg.bias_init,\n",
    "        name='embed_x')(x)\n",
    "    dx = nn.Dense(\n",
    "        cfg.hidden_dim // 2,\n",
    "        kernel_init=cfg.kernel_init,\n",
    "        bias_init=cfg.bias_init,\n",
    "        name='embed_dx')(dx)\n",
    "    x = jnp.concatenate([x, dx], axis=-1)\n",
    "    assert x.shape[-1] == cfg.hidden_dim, f\"{x.shape[-1]} != {cfg.hidden_dim}\"\n",
    "\n",
    "    x = nn.Dropout(rate=cfg.dropout_rate)(x, deterministic=deterministic)\n",
    "    x = AddPositionEmbs(config=cfg, name='posembed_output')(x)\n",
    "    x = nn.Dropout(rate=cfg.dropout_rate)(x, deterministic=deterministic)\n",
    "\n",
    "    # Target-Input Decoder\n",
    "    for lyr in range(cfg.num_layers):\n",
    "      x = EncoderDecoder1DBlock(\n",
    "          config=cfg, name=f'encoderdecoderblock_{lyr}')(\n",
    "              x,\n",
    "              deterministic=deterministic,\n",
    "              decoder_mask=decoder_mask)\n",
    "    x = nn.LayerNorm(name='encoderdecoder_norm')(x)\n",
    "    logits_x = nn.Dense(\n",
    "        cfg.output_size,\n",
    "        kernel_init=cfg.kernel_init,\n",
    "        bias_init=cfg.bias_init,\n",
    "        name='logits_x')(x)\n",
    "    logits_dx = nn.Dense(\n",
    "        cfg.output_size,\n",
    "        kernel_init=cfg.kernel_init,\n",
    "        bias_init=cfg.bias_init,\n",
    "        name='logits_dx')(x)\n",
    "    return logits_x, logits_dx, None\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  \"\"\"Transformer pure decoder stack for language modelling.\n",
    "\n",
    "  Args:\n",
    "    config: TransformerConfig dataclass containing hyperparameters.\n",
    "  \"\"\"\n",
    "  config: TransformerConfig\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self,\n",
    "               inputs,\n",
    "               train):\n",
    "    \"\"\"Applies Transformer on the inputs.\n",
    "\n",
    "    N.b. does not support masking for incomplete sequences.\n",
    "\n",
    "    Args:\n",
    "      inputs: target data.\n",
    "\n",
    "    Returns:\n",
    "      logits array from transformer decoder.\n",
    "    \"\"\"\n",
    "    assert inputs['x'].ndim == 3  # (batch, len, channels)\n",
    "    assert inputs['y'].ndim == 3  # (batch, len, channels)\n",
    "    assert inputs['x'].shape[1] == inputs['y'].shape[1]\n",
    "\n",
    "    cfg = self.config\n",
    "    decoder_mask = None\n",
    "    if cfg.causal_x:\n",
    "      decoder_mask = nn.make_causal_mask(inputs['x'][...,0])\n",
    "    if cfg.physics_decoder:\n",
    "      logits_x, logits_dx, aux = PhysicsDecoder(config=cfg, name='decoder')(\n",
    "          inputs,\n",
    "          deterministic=not train,\n",
    "          decoder_mask=decoder_mask)\n",
    "    else:\n",
    "      logits_x, logits_dx, aux = Decoder(config=cfg, name='decoder')(\n",
    "          inputs,\n",
    "          deterministic=not train,\n",
    "          decoder_mask=decoder_mask)\n",
    "\n",
    "    return logits_x, logits_dx, aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ad91b36-43a0-452f-8dea-f07acfd92e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_l2(predictions, targets, padding):\n",
    "  \"\"\"Compute weighted cross entropy and entropy for log probs and targets.\n",
    "  Args:\n",
    "   predictions: [batch, length, dim] float array.\n",
    "   targets: categorical targets [batch, length, dim] float array.\n",
    "   padding: [batch, length] padding mask (1=padding)\n",
    "  Returns:\n",
    "    Tuple of scalar loss and batch normalizing factor.\n",
    "  \"\"\"\n",
    "  if predictions.ndim != targets.ndim :\n",
    "    raise ValueError('Incorrect shapes. Got shape %s predictions and %s targets' %\n",
    "                     (str(predictions.shape), str(targets.shape)))\n",
    "\n",
    "  predictions = predictions * (1 - padding[...,None])\n",
    "  targets = targets * (1 - padding[...,None])\n",
    "  loss = jnp.sum((predictions - targets) ** 2, axis=-1)  # Sum over channels\n",
    "  return loss.mean()   # Per-timestep average loss.\n",
    "\n",
    "\n",
    "def compute_hinge(values):\n",
    "  \"\"\"Compute hinge loss.\n",
    "  Args:\n",
    "   predictions: [batch, length] float array.\n",
    "  Returns:\n",
    "    Loss\n",
    "  \"\"\"\n",
    "  assert len(values.shape) == 2, f\"{len(values.shape)} != 2\"\n",
    "  loss = jnp.clip(values, a_min=0)\n",
    "  return loss.mean()   # Mean over time and batch.\n",
    "\n",
    "\n",
    "def compute_loss(p1, p2, aux, labels1, labels2, pad, alpha=0.0, aux_loss_weight=0.0):\n",
    "  l1 = compute_l2(p1, labels1, pad)\n",
    "  l2 = compute_l2(p2, labels2, pad)\n",
    "  l2_loss = (1 - alpha) * l1 + alpha * l2\n",
    "  aux_loss = 0.0\n",
    "  if aux is not None:\n",
    "    aux_loss = compute_hinge(aux)\n",
    "  else:\n",
    "    assert aux_loss_weight == 0.0\n",
    "  return l2_loss + aux_loss_weight * aux_loss, l2_loss, aux_loss\n",
    "\n",
    "\n",
    "def compute_metrics(p1, p2, aux, labels1, labels2, pad, alpha, aux_loss_weight):\n",
    "  \"\"\"Compute summary metrics.\"\"\"\n",
    "  loss, l2, aux = compute_loss(p1, p2, aux, labels1, labels2, pad, alpha, aux_loss_weight)\n",
    "  metrics ={\n",
    "      'loss': loss,\n",
    "      'l2_loss': l2,\n",
    "      'aux_loss': aux,\n",
    "  }\n",
    "  metrics = jax.lax.psum(metrics, axis_name=\"batch\")\n",
    "  return metrics\n",
    "\n",
    "\n",
    "def build_deltas(x):\n",
    "  dx = x[:, 1:, :] - x[:, :-1, :]\n",
    "  b, _, c = x.shape\n",
    "  first_dx = jnp.zeros(shape=(b, 1, c), dtype=jnp.float32)\n",
    "  dx = jnp.concatenate([first_dx, dx], axis=1)\n",
    "  return dx\n",
    "\n",
    "\n",
    "def train_step(state, inputs, model, learning_rate_fn, alpha, aux_loss_weight,\n",
    "               dropout_rng=None):\n",
    "  \"\"\"Perform a single training step.\"\"\"\n",
    "\n",
    "  dropout_rng = jax.random.fold_in(dropout_rng, state.step)\n",
    "  y = inputs['y']\n",
    "  dy = build_deltas(y)\n",
    "  pad = inputs['pad']\n",
    "\n",
    "  def loss_fn(params):\n",
    "    \"\"\"loss function used for training.\"\"\"\n",
    "    py, pdy, aux = model.apply(\n",
    "        {'params': params},\n",
    "        inputs,\n",
    "        train=True,\n",
    "        rngs={\"dropout\": dropout_rng})\n",
    "    loss = compute_loss(py, pdy, aux, y, dy, pad, alpha, aux_loss_weight)[0]\n",
    "    return loss, (py, pdy, aux)\n",
    "\n",
    "  step = state.step\n",
    "  lr = learning_rate_fn(step)\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  aux, grads = grad_fn(state.params)\n",
    "  _, (p1, p2, aux) = aux\n",
    "  grads = jax.lax.pmean(grads, 'batch')\n",
    "  new_state = state.apply_gradients(grads=grads)\n",
    "\n",
    "  metrics = compute_metrics(p1, p2, aux, y, dy, pad, alpha, aux_loss_weight)\n",
    "  metrics['learning_rate'] = lr\n",
    "\n",
    "  return new_state, metrics\n",
    "\n",
    "\n",
    "def eval_step(params, inputs, model, alpha, aux_loss_weight):\n",
    "  \"\"\"Calculate evaluation metrics on a inputs.\"\"\"\n",
    "  y = inputs['y']\n",
    "  dy = build_deltas(y)\n",
    "  pad = inputs['pad']\n",
    "  py, pdy, aux = model.apply({'params': params}, inputs, train=False)\n",
    "  return compute_metrics(py, pdy, aux, y, dy, pad, alpha, aux_loss_weight)\n",
    "\n",
    "\n",
    "# Call a jitted initialization function to get the initial parameter tree.\n",
    "@functools.partial(jax.jit, static_argnums=[0, 1])\n",
    "def initialize_variables(config, model, init_rng):\n",
    "  example = jnp.ones((1, config.max_len, 1), jnp.float32)\n",
    "  init_batch = {'x': example, 'y': example}\n",
    "  init_variables = model.init(init_rng, inputs=init_batch, train=False)\n",
    "  return init_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2321d464-b7de-4f02-9674-ebcb6156329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {}\n",
    "\n",
    "# Setup\n",
    "hparams['model_dir'] = '/tmp/test'\n",
    "\n",
    "# Model params\n",
    "hparams['physics_decoder'] = False\n",
    "hparams['max_len'] = MAX_LEN\n",
    "hparams['num_layers'] = 4\n",
    "hparams['hidden_dim'] = 16\n",
    "hparams['mlp_dim'] = 4 * hparams['hidden_dim']\n",
    "hparams['num_heads'] = 2\n",
    "hparams['dropout_rate'] = 0.0\n",
    "hparams['attention_dropout_rate'] = 0.0\n",
    "hparams['alpha'] = 0.0\n",
    "hparams['aux_loss_weight'] = 0.0\n",
    "hparams['causal_x'] = True\n",
    "\n",
    "# Training params\n",
    "hparams['batch_size'] = 16\n",
    "hparams['learning_rate'] = 0.01\n",
    "hparams['num_train_steps'] = 1001\n",
    "hparams['eval_freq'] = 100\n",
    "hparams['weight_decay'] = 0.0\n",
    "hparams['random_seed'] = 0\n",
    "\n",
    "# Make sure tf does not allocate gpu memory.\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "\n",
    "if hparams['batch_size'] % jax.device_count() > 0:\n",
    "  raise ValueError('Batch size must be divisible by the number of devices')\n",
    "device_batch_size = hparams['batch_size'] // jax.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4fdf9e32-e1b8-4d5a-b2d6-8bf2f9fbe742",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object __array__ method not producing an array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_ds, eval_ds \u001b[38;5;241m=\u001b[39m \u001b[43mget_datasets_tf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_ds)\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m TransformerConfig(\n\u001b[1;32m      5\u001b[0m     max_len\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_len\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     physics_decoder\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphysics_decoder\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     14\u001b[0m )\n",
      "Cell \u001b[0;32mIn[46], line 21\u001b[0m, in \u001b[0;36mget_datasets_tf\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_datasets_tf\u001b[39m(batch_size):\n\u001b[1;32m     20\u001b[0m   train_np, test_np \u001b[38;5;241m=\u001b[39m get_datasets_np()\n\u001b[0;32m---> 21\u001b[0m   train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m   train_ds \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m     23\u001b[0m   train_ds \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:826\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py:134\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    133\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 134\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:713\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    712\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    293\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: object __array__ method not producing an array"
     ]
    }
   ],
   "source": [
    "train_ds, eval_ds = get_datasets_tf(batch_size=hparams['batch_size'])\n",
    "train_iter = iter(train_ds)\n",
    "\n",
    "config = TransformerConfig(\n",
    "    max_len=hparams['max_len'],\n",
    "    num_layers=hparams['num_layers'],\n",
    "    hidden_dim=hparams['hidden_dim'],\n",
    "    mlp_dim=hparams['mlp_dim'],\n",
    "    num_heads=hparams['num_heads'],\n",
    "    dropout_rate=hparams['dropout_rate'],\n",
    "    attention_dropout_rate=hparams['attention_dropout_rate'],\n",
    "    causal_x=hparams['causal_x'],\n",
    "    physics_decoder=hparams['physics_decoder'],\n",
    ")\n",
    "\n",
    "rng = jax.random.PRNGKey(hparams['random_seed'])\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "model = Transformer(config)\n",
    "init_variables = initialize_variables(config, model, init_rng)\n",
    "\n",
    "learning_rate_fn = create_learning_rate_scheduler(\n",
    "    base_learning_rate=hparams['learning_rate'])\n",
    "\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate_fn, b1=0.9, b2=0.98, eps=1e-9,\n",
    "    weight_decay=1e-1)\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=init_variables[\"params\"],\n",
    "    tx=optimizer)\n",
    "state = flax.jax_utils.replicate(state)\n",
    "\n",
    "p_train_step = jax.pmap(\n",
    "    functools.partial(\n",
    "        train_step,\n",
    "        model=model,\n",
    "        learning_rate_fn=learning_rate_fn,\n",
    "        alpha=hparams['alpha'],\n",
    "        aux_loss_weight=hparams['aux_loss_weight']),\n",
    "    axis_name='batch',\n",
    "    donate_argnums=(0,))  # pytype: disable=wrong-arg-types\n",
    "p_eval_step = jax.pmap(\n",
    "    functools.partial(\n",
    "        eval_step,\n",
    "        model=model,\n",
    "        alpha=hparams['alpha'],\n",
    "        aux_loss_weight=hparams['aux_loss_weight']),\n",
    "    axis_name='batch')\n",
    "\n",
    "# We init the first set of dropout PRNG keys, but update it afterwards inside\n",
    "# the main pmap'd training update for performance.\n",
    "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
    "del rng\n",
    "\n",
    "metrics_all = []\n",
    "tick = time.time()\n",
    "best_dev_score = 1e6\n",
    "for step, batch in zip(range(hparams['num_train_steps']), train_iter):\n",
    "  batch = flax.training.common_utils.shard(\n",
    "      jax.tree.map(lambda x: x._numpy(), batch))  # pylint: disable=protected-access\n",
    "\n",
    "  state, metrics = p_train_step(state, batch, dropout_rng=dropout_rngs)\n",
    "  metrics_all.append(metrics)\n",
    "\n",
    "  if step % hparams['eval_freq'] == 0:\n",
    "    metrics_all = flax.training.common_utils.get_metrics(metrics_all)\n",
    "    lr = metrics_all.pop('learning_rate').mean()\n",
    "    summary = jax.tree.map(jnp.mean, metrics_all)\n",
    "    summary['learning_rate'] = lr\n",
    "    metrics_all = []\n",
    "\n",
    "    if jax.process_index() == 0:\n",
    "      tock = time.time()\n",
    "      steps_per_sec = hparams['eval_freq'] / (tock - tick)\n",
    "      logging.info('Steps per second: %.1f', steps_per_sec)\n",
    "      tick = tock\n",
    "\n",
    "    eval_metrics = []\n",
    "    eval_iter = iter(eval_ds)\n",
    "    for i, eval_batch in enumerate(eval_iter):\n",
    "      eval_batch = jax.tree.map(lambda x: x._numpy(), eval_batch)  # pylint: disable=protected-access\n",
    "      # TODO: Handle final odd-sized batch by padding instead of dropping it.\n",
    "      cur_pred_batch_size = eval_batch['x'].shape[0]\n",
    "      if cur_pred_batch_size != hparams['batch_size']:\n",
    "        continue\n",
    "      eval_batch = flax.training.common_utils.shard(eval_batch)\n",
    "\n",
    "      metrics_from_eval = p_eval_step(state.params, eval_batch)\n",
    "      eval_metrics.append(metrics_from_eval)\n",
    "    eval_metrics = flax.training.common_utils.get_metrics(eval_metrics)\n",
    "    eval_summary = jax.tree.map(jnp.mean, eval_metrics)\n",
    "\n",
    "    print('Step: %04d,\\ttrain loss %0.4f,\\ttrain l2 %0.4f,\\ttrain aux %0.4f,\\teval loss %0.4f,\\teval l2 %0.4f,\\teval aux %0.4f,\\tsteps/s %0.1f' % (\n",
    "        step,\n",
    "        summary['loss'],\n",
    "        summary['l2_loss'],\n",
    "        summary['aux_loss'],\n",
    "        eval_summary['loss'],\n",
    "        eval_summary['l2_loss'],\n",
    "        eval_summary['aux_loss'],\n",
    "        steps_per_sec))\n",
    "\n",
    "    if best_dev_score > eval_summary['loss']:\n",
    "      best_dev_score = eval_summary['loss']\n",
    "      best_state = state\n",
    "\n",
    "    eval_summary['best_dev_score'] = best_dev_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "155d2d56-aeb3-4757-806b-db09aa0f2d98",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object __array__ method not producing an array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_np, test_np \u001b[38;5;241m=\u001b[39m get_datasets_np()\n\u001b[0;32m----> 2\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# train_np\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:826\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# from_tensor_slices_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensor_slices_op\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensor_slices_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:25\u001b[0m, in \u001b[0;36m_from_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/ops/from_tensor_slices_op.py:33\u001b[0m, in \u001b[0;36m_TensorSliceDataset.__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[1;32m     35\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/data/util/structure.py:134\u001b[0m, in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    133\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 134\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:713\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    712\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    293\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: object __array__ method not producing an array"
     ]
    }
   ],
   "source": [
    "train_np, test_np = get_datasets_np()\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train_np)\n",
    "# train_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "26cf47dc-25ec-4737-96a2-bc47cf7345dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted to numpy array\n",
      "Shape: (136, 1024, 1)\n",
      "Data type: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "try:\n",
    "    x_array = np.array(train_np['x'])\n",
    "    print(\"Successfully converted to numpy array\")\n",
    "    print(\"Shape:\", x_array.shape)\n",
    "    print(\"Data type:\", x_array.dtype)\n",
    "except Exception as e:\n",
    "    print(\"Error converting to numpy array:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "26d1739d-b6bd-4b93-a341-f87839ab344c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 2.0.0\n",
      "TensorFlow version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e815ce30-abfa-4593-b202-701ac77f6242",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array = np.array(train_np['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "383bed99-83b7-4d2a-8e71-03232193e076",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object __array__ method not producing an array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tf_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: object __array__ method not producing an array"
     ]
    }
   ],
   "source": [
    "tf_tensor = tf.convert_to_tensor(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3015fc30-4a02-4e3d-93ab-85cd9e08221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting new array to TensorFlow tensor: object __array__ method not producing an array\n"
     ]
    }
   ],
   "source": [
    "new_array = np.array(train_np['x'])\n",
    "try:\n",
    "    tf_tensor = tf.convert_to_tensor(new_array)\n",
    "    print(\"Successfully converted new array to TensorFlow tensor\")\n",
    "except Exception as e:\n",
    "    print(\"Error converting new array to TensorFlow tensor:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1ed96a6b-b043-4f10-b0a6-524c75a98315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting float32 array to TensorFlow tensor: object __array__ method not producing an array\n"
     ]
    }
   ],
   "source": [
    "new_array_float32 = new_array.astype(np.float32)\n",
    "try:\n",
    "    tf_tensor = tf.convert_to_tensor(new_array_float32)\n",
    "    print(\"Successfully converted float32 array to TensorFlow tensor\")\n",
    "except Exception as e:\n",
    "    print(\"Error converting float32 array to TensorFlow tensor:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a51f225b-8281-4ad1-a91b-45b2d5362836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting slice of data to TensorFlow tensor: object __array__ method not producing an array\n",
      "Error converting final reshaped data to TensorFlow tensor: object __array__ method not producing an array\n"
     ]
    }
   ],
   "source": [
    "data = RAW_DATA.copy()\n",
    "b, l, _ = data.shape\n",
    "assert MAX_LEN >= l\n",
    "data = np.pad(data, ([0,0], [0, MAX_LEN-l], [0,0]))\n",
    "\n",
    "try:\n",
    "    tf_tensor = tf.convert_to_tensor(data[:TRAIN_SIZE, :MAX_LEN, 0])\n",
    "    print(\"Successfully converted slice of data to TensorFlow tensor\")\n",
    "except Exception as e:\n",
    "    print(\"Error converting slice of data to TensorFlow tensor:\", e)\n",
    "\n",
    "try:\n",
    "    tf_tensor = tf.convert_to_tensor(data[:TRAIN_SIZE, :MAX_LEN, 0][...,None])\n",
    "    print(\"Successfully converted final reshaped data to TensorFlow tensor\")\n",
    "except Exception as e:\n",
    "    print(\"Error converting final reshaped data to TensorFlow tensor:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "781aaba4-bd9c-4a40-8e2e-0f07b20fa3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting random array to TensorFlow tensor: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    random_array = np.random.rand(*new_array.shape).astype(new_array.dtype)\n",
    "    tf_tensor = tf.convert_to_tensor(random_array)\n",
    "    print(\"Successfully converted random array to TensorFlow tensor\")\n",
    "except Exception as e:\n",
    "    print(\"Error converting random array to TensorFlow tensor:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b279d845-9826-4827-a943-27ba39c09005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
      "NumPy version: 2.0.0\n",
      "TensorFlow version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
