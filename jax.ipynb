{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74cb9bde-2015-4568-ab61-17980d6fa4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 15:53:32.440733: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-19 15:53:32.450076: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-19 15:53:32.452995: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-19 15:53:32.904746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#@title Imports { form-width: \"10%\" }\n",
    "\n",
    "from typing import Callable, Any, Optional\n",
    "import flax\n",
    "import flax.training.common_utils\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import pdb\n",
    "\n",
    "from absl import logging\n",
    "from flax.training import train_state\n",
    "from jax import lax\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "print(jax.devices())\n",
    "print(jax.default_backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84fea4de-56c9-4961-8ab0-b5cfd22b2908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1001, 2)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data.npz'\n",
    "\n",
    "with open(DATA_PATH, 'rb') as f:\n",
    "  data = np.load(f)\n",
    "  RAW_DATA = data[\"arr_0\"]\n",
    "\n",
    "# D_RAW_DATA = RAW_DATA[:,1:,:] - RAW_DATA[:,:-1,:]\n",
    "# D_RAW_DATA = np.pad(D_RAW_DATA, [(0,0), (1,0), (0,0)])\n",
    "\n",
    "print(RAW_DATA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1237f59-7077-4f8e-9b42-ba468d63b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 1024\n",
    "TRAIN_SIZE = 200 - 64\n",
    "PAD_VALUE = -1e10\n",
    "is_pad = lambda x : np.isclose(x, PAD_VALUE)\n",
    "\n",
    "def get_datasets_np(data=RAW_DATA, max_len=MAX_LEN, train_size=TRAIN_SIZE):\n",
    "  b, l, _ = data.shape\n",
    "  assert max_len >= l\n",
    "  data = np.pad(data, ([0,0], [0, max_len-l], [0,0]))\n",
    "\n",
    "  return {\n",
    "      'x': data[:TRAIN_SIZE, :MAX_LEN, 0][...,None],\n",
    "      'y': data[:TRAIN_SIZE, :MAX_LEN, 1][...,None],\n",
    "      'pad': is_pad(data[:TRAIN_SIZE, :MAX_LEN, 0])\n",
    "  }, {\n",
    "      'x': data[TRAIN_SIZE:, :MAX_LEN, 0][...,None],\n",
    "      'y': data[TRAIN_SIZE:, :MAX_LEN, 1][...,None],\n",
    "      'pad': is_pad(data[TRAIN_SIZE:, :MAX_LEN, 0])\n",
    "  }\n",
    "\n",
    "def get_datasets_tf(batch_size):\n",
    "  train_np, test_np = get_datasets_np()\n",
    "  train_ds = tf.data.Dataset.from_tensor_slices(train_np)\n",
    "  train_ds = train_ds.cache()\n",
    "  train_ds = train_ds.repeat(None)\n",
    "  train_ds = train_ds.batch(batch_size)\n",
    "  train_ds = train_ds.prefetch(4)\n",
    "  eval_ds = tf.data.Dataset.from_tensor_slices(test_np)\n",
    "  eval_ds = eval_ds.cache()\n",
    "  eval_ds = eval_ds.repeat(1)\n",
    "  eval_ds = eval_ds.batch(batch_size)\n",
    "  eval_ds = eval_ds.prefetch(4)\n",
    "  return train_ds, eval_ds\n",
    "\n",
    "\n",
    "def create_learning_rate_scheduler(\n",
    "    factors='constant * linear_warmup * rsqrt_decay',\n",
    "    base_learning_rate=0.5,\n",
    "    warmup_steps=8000,\n",
    "    decay_factor=0.5,\n",
    "    steps_per_decay=20000,\n",
    "    steps_per_cycle=100000):\n",
    "  \"\"\"creates learning rate schedule.\n",
    "  Interprets factors in the factors string which can consist of:\n",
    "  * constant: interpreted as the constant value,\n",
    "  * linear_warmup: interpreted as linear warmup until warmup_steps,\n",
    "  * rsqrt_decay: divide by square root of max(step, warmup_steps)\n",
    "  * decay_every: Every k steps decay the learning rate by decay_factor.\n",
    "  * cosine_decay: Cyclic cosine decay, uses steps_per_cycle parameter.\n",
    "  Args:\n",
    "    factors: a string with factors separated by '*' that defines the schedule.\n",
    "    base_learning_rate: float, the starting constant for the lr schedule.\n",
    "    warmup_steps: how many steps to warm up for in the warmup schedule.\n",
    "    decay_factor: The amount to decay the learning rate by.\n",
    "    steps_per_decay: How often to decay the learning rate.\n",
    "    steps_per_cycle: Steps per cycle when using cosine decay.\n",
    "  Returns:\n",
    "    a function learning_rate(step): float -> {'learning_rate': float}, the\n",
    "    step-dependent lr.\n",
    "  \"\"\"\n",
    "  factors = [n.strip() for n in factors.split('*')]\n",
    "\n",
    "  def step_fn(step):\n",
    "    \"\"\"Step to learning rate functio.\"\"\"\n",
    "    ret = 1.0\n",
    "    for name in factors:\n",
    "      if name == 'constant':\n",
    "        ret *= base_learning_rate\n",
    "      elif name == 'linear_warmup':\n",
    "        ret *= jnp.minimum(1.0, step / warmup_steps)\n",
    "      elif name == 'rsqrt_decay':\n",
    "        ret /= jnp.sqrt(jnp.maximum(step, warmup_steps))\n",
    "      elif name == 'rsqrt_normalized_decay':\n",
    "        ret *= jnp.sqrt(warmup_steps)\n",
    "        ret /= jnp.sqrt(jnp.maximum(step, warmup_steps))\n",
    "      elif name == 'decay_every':\n",
    "        ret *= (decay_factor**(step // steps_per_decay))\n",
    "      elif name == 'cosine_decay':\n",
    "        progress = jnp.maximum(0.0,\n",
    "                               (step - warmup_steps) / float(steps_per_cycle))\n",
    "        ret *= jnp.maximum(0.0,\n",
    "                           0.5 * (1.0 + jnp.cos(jnp.pi * (progress % 1.0))))\n",
    "      else:\n",
    "        raise ValueError('Unknown factor %s.' % name)\n",
    "    return ret\n",
    "\n",
    "  return step_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e35c695c-361f-4257-9f84-06b1ed917685",
   "metadata": {},
   "outputs": [],
   "source": [
    "@flax.struct.dataclass\n",
    "class TransformerConfig:\n",
    "  \"\"\"Global hyperparameters used to minimize kwarg plumbing.\"\"\"\n",
    "  output_size: int = 1\n",
    "  max_len: int = MAX_LEN\n",
    "  num_layers: int = 2\n",
    "  hidden_dim: int = 16\n",
    "  mlp_dim: int = 64\n",
    "  num_heads: int = 4\n",
    "  dropout_rate: float = 0.0\n",
    "  attention_dropout_rate: float = 0.0\n",
    "  deterministic: bool = False\n",
    "  decode: bool = False\n",
    "  causal_x: bool = True\n",
    "  physics_decoder: bool = False\n",
    "  kernel_init: Callable = nn.initializers.xavier_uniform()\n",
    "  bias_init: Callable = nn.initializers.normal(stddev=1e-6)\n",
    "  posemb_init: Callable = nn.initializers.normal(stddev=0.02)\n",
    "\n",
    "\n",
    "def shift_right(x, axis=1):\n",
    "  \"\"\"Shift the input to the right by padding and slicing on axis.\"\"\"\n",
    "  pad_widths = [(0, 0)] * len(x.shape)\n",
    "  pad_widths[axis] = (1, 0)\n",
    "  padded = jnp.pad(\n",
    "      x, pad_widths, mode='constant', constant_values=x.dtype.type(0))\n",
    "  return lax.dynamic_slice_in_dim(padded, 0, padded.shape[axis] - 1, axis)\n",
    "\n",
    "\n",
    "\n",
    "class AddPositionEmbs(nn.Module):\n",
    "  \"\"\"Adds (optionally learned) positional embeddings to the inputs.\n",
    "\n",
    "  Attributes:\n",
    "    config: TransformerConfig dataclass containing hyperparameters.\n",
    "  \"\"\"\n",
    "  config: TransformerConfig\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs):\n",
    "    \"\"\"Applies AddPositionEmbs module.\n",
    "\n",
    "    By default this layer uses a fixed sinusoidal embedding table. If a\n",
    "    learned position embedding is desired, pass an initializer to\n",
    "    posemb_init in the configuration.\n",
    "\n",
    "    Args:\n",
    "      inputs: input data.\n",
    "    Returns:\n",
    "      output: `(bs, timesteps, in_dim)`\n",
    "    \"\"\"\n",
    "    cfg = self.config\n",
    "    # inputs.shape is (batch_size, seq_len, hidden_dim)\n",
    "    assert inputs.ndim == 3, ('Number of dimensions should be 3,'\n",
    "                              ' but it is: %d' % inputs.ndim)\n",
    "    length = inputs.shape[1]\n",
    "    pos_emb_shape = (1, cfg.max_len, inputs.shape[-1])\n",
    "    pos_embedding = self.param('pos_embedding',\n",
    "                               nn.initializers.normal(stddev=0.02),\n",
    "                               pos_emb_shape)\n",
    "    return inputs + pos_embedding[:, :length, :]\n",
    "\n",
    "\n",
    "class MlpBlock(nn.Module):\n",
    "  \"\"\"Transformer MLP / feed-forward block.\n",
    "  Attributes:\n",
    "    config: TransformerConfig dataclass containing hyperparameters.\n",
    "    out_dim: optionally specify out dimension.\n",
    "  \"\"\"\n",
    "  config: TransformerConfig\n",
    "  out_dim: Optional[int] = None\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs, deterministic=True):\n",
    "    \"\"\"Applies Transformer MlpBlock module.\"\"\"\n",
    "    cfg = self.config\n",
    "    actual_out_dim = (inputs.shape[-1] if self.out_dim is None\n",
    "                      else self.out_dim)\n",
    "    x = nn.Dense(cfg.mlp_dim,\n",
    "                   kernel_init=cfg.kernel_init,\n",
    "                   bias_init=cfg.bias_init)(inputs)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dropout(rate=cfg.dropout_rate)(x, deterministic=deterministic)\n",
    "    output = nn.Dense(actual_out_dim,\n",
    "                         kernel_init=cfg.kernel_init,\n",
    "                         bias_init=cfg.bias_init)(x)\n",
    "    output = nn.Dropout(rate=cfg.dropout_rate)(\n",
    "        output, deterministic=deterministic)\n",
    "    return output\n",
    "\n",
    "\n",
    "class EncoderDecoder1DBlock(nn.Module):\n",
    "  \"\"\"Transformer encoder-decoder layer.\n",
    "\n",
    "  Args:\n",
    "    config: TransformerConfig dataclass containing hyperparameters.\n",
    "  \"\"\"\n",
    "  config: TransformerConfig\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self,\n",
    "               inputs,\n",
    "               deterministic,\n",
    "               decoder_mask=None):\n",
    "    \"\"\"Applies EncoderDecoder1DBlock module.\n",
    "\n",
    "    Args:\n",
    "      inputs: input data for decoder\n",
    "      decoder_mask: decoder self-attention mask.\n",
    "\n",
    "    Returns:\n",
    "      output after transformer encoder-decoder block.\n",
    "    \"\"\"\n",
    "    cfg = self.config\n",
    "\n",
    "    # Decoder block.\n",
    "    assert inputs.ndim == 3, ('Number of dimensions should be 3,'\n",
    "                              ' but it is: %d' % inputs.ndim)\n",
    "    x = nn.LayerNorm()(inputs)\n",
    "    x = nn.SelfAttention(\n",
    "        num_heads=cfg.num_heads,\n",
    "        qkv_features=cfg.hidden_dim,\n",
    "        kernel_init=cfg.kernel_init,\n",
    "        bias_init=cfg.bias_init,\n",
    "        use_bias=False,\n",
    "        broadcast_dropout=False,\n",
    "        dropout_rate=cfg.attention_dropout_rate,\n",
    "        deterministic=deterministic)(x, mask=decoder_mask)\n",
    "    x = nn.Dropout(rate=cfg.dropout_rate)(\n",
    "        x, deterministic=deterministic)\n",
    "    x = x + inputs\n",
    "\n",
    "    # MLP block.\n",
    "    z = nn.LayerNorm()(x)\n",
    "    z = MlpBlock(config=cfg)(z)\n",
    "\n",
    "    return x + z\n",
    "\n",
    "\n",
    "class PhysicsDecoder(nn.Module):\n",
    "  \"\"\"Transformer Model Decoder for sequence to sequence translation.\n",
    "\n",
    "  Args:\n",
    "    config: TransformerConfig dataclass containing hyperparameters.\n",
    "  \"\"\"\n",
    "  config: TransformerConfig\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self,\n",
    "               inputs,\n",
    "               deterministic,\n",
    "               decoder_mask=None):\n",
    "    \"\"\"Applies Transformer model on the inputs.\n",
    "\n",
    "    Args:\n",
    "      encoded: encoded input data from encoder.\n",
    "      inputs: input data.\n",
    "      decoder_mask: decoder self-attention mask.\n",
    "\n",
    "    Returns:\n",
    "      output of a transformer decoder.\n",
    "    \"\"\"\n",
    "    x = inputs['x']\n",
    "    dx = build_deltas(x)\n",
    "\n",
    "    cfg = self.config\n",
    "    assert cfg.alpha == 0.0\n",
    "\n",
    "    x = nn.Dense(\n",
    "        cfg.hidden_dim // 2,\n",
    "        kernel_init=cfg.kernel_init,\n",
    "        bias_init=cfg.bias_init,\n",
    "        name='embed_x')(x)\n",
    "    dx = nn.Dense(\n",
    "        cfg.hidden_dim // 2,\n",
    "        kernel_init=cfg.kernel_init,\n",
    "        bias_init=cfg.bias_init,\n",
    "        name='embed_dx')(dx)\n",
    "    x = jnp.concatenate([x, dx], axis=-1)\n",
    "    assert x.shape[-1] == cfg.hidden_dim, f\"{x.shape[-1]} != {cfg.hidden_dim}\"\n",
    "\n",
    "    x = nn.Dropout(rate=cfg.dropout_rate)(x, deterministic=deterministic)\n",
    "    x = AddPositionEmbs(config=cfg, name='posembed_output')(x)\n",
    "    x = nn.Dropout(rate=cfg.dropout_rate)(x, deterministic=deterministic)\n",
    "\n",
    "    # Target-Input Decoder\n",
    "    for lyr in range(cfg.num_layers):\n",
    "      x = EncoderDecoder1DBlock(\n",
    "          config=cfg, name=f'encoderdecoderblock_{lyr}')(\n",
    "              x,\n",
    "              deterministic=deterministic,\n",
    "              decoder_mask=decoder_mask)\n",
    "    alphas = nn.LayerNorm(name='alphas')(x)\n",
    "\n",
    "    dfdx = jax.jacfwd(self.f, argnums=0)\n",
    "    dfdalpha = jax.jacfwd(self.f, argnums=1)\n",
    "\n",
    "    x = dfdx(inputs['x'], alphas, cfg)\n",
    "    x = jnp.einsum('abcdbf->abf', x)\n",
    "    dfda = dfdalpha(inputs['x'], alphas, cfg)  # [B,S,C]\n",
    "    dfda = jnp.einsum('abcdbf->abf', dfda)\n",
    "\n",
    "    delta_a = alphas[:,1:,:] - alphas[:,:-1,:]  # [B,S-1,C]\n",
    "    first_delta_a = jnp.zeros([delta_a.shape[0], 1, delta_a.shape[-1]])\n",
    "    delta_a = jnp.concatenate([first_delta_a, delta_a], axis=1)  # [B,S-1,C]\n",
    "    aux = - jnp.sum(dfda * delta_a, axis=-1)  # [B,S]\n",
    "\n",
    "    # prediction (y/sigma), aux value > 0\n",
    "    return x, jnp.zeros_like(x), aux\n",
    "\n",
    "  def f(self, x, alphas, cfg):\n",
    "    \"\"\"alphas [B,S,C], x [X,S,1]\"\"\"\n",
    "    x = jnp.concatenate([alphas, x], axis=-1)\n",
    "    for _ in range(2):\n",
    "      x = nn.Dense(\n",
    "          16,\n",
    "          kernel_init=cfg.kernel_init,\n",
    "          bias_init=cfg.bias_init)(x)\n",
    "      x = nn.LayerNorm()(x)\n",
    "    x = nn.Dense(\n",
    "          1,\n",
    "          kernel_init=cfg.kernel_init,\n",
    "          bias_init=cfg.bias_init)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  \"\"\"Transformer Model Decoder for sequence to sequence translation.\n",
    "\n",
    "  Args:\n",
    "    config: TransformerConfig dataclass containing hyperparameters.\n",
    "  \"\"\"\n",
    "  config: TransformerConfig\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self,\n",
    "               inputs,\n",
    "               deterministic,\n",
    "               decoder_mask=None):\n",
    "    \"\"\"Applies Transformer model on the inputs.\n",
    "\n",
    "    Args:\n",
    "      encoded: encoded input data from encoder.\n",
    "      inputs: input data.\n",
    "      decoder_mask: decoder self-attention mask.\n",
    "\n",
    "    Returns:\n",
    "      output of a transformer decoder.\n",
    "    \"\"\"\n",
    "    x = inputs['x']\n",
    "    dx = build_deltas(x)\n",
    "    # y = inputs['y']\n",
    "\n",
    "    cfg = self.config\n",
    "    # if not cfg.decode:\n",
    "      # y = shift_right(y)\n",
    "\n",
    "    x = nn.Dense(\n",
    "        cfg.hidden_dim // 2,\n",
    "        kernel_init=cfg.kernel_init,\n",
    "        bias_init=cfg.bias_init,\n",
    "        name='embed_x')(x)\n",
    "    dx = nn.Dense(\n",
    "        cfg.hidden_dim // 2,\n",
    "        kernel_init=cfg.kernel_init,\n",
    "        bias_init=cfg.bias_init,\n",
    "        name='embed_dx')(dx)\n",
    "    x = jnp.concatenate([x, dx], axis=-1)\n",
    "    assert x.shape[-1] == cfg.hidden_dim, f\"{x.shape[-1]} != {cfg.hidden_dim}\"\n",
    "\n",
    "    x = nn.Dropout(rate=cfg.dropout_rate)(x, deterministic=deterministic)\n",
    "    x = AddPositionEmbs(config=cfg, name='posembed_output')(x)\n",
    "    x = nn.Dropout(rate=cfg.dropout_rate)(x, deterministic=deterministic)\n",
    "\n",
    "    # Target-Input Decoder\n",
    "    for lyr in range(cfg.num_layers):\n",
    "      x = EncoderDecoder1DBlock(\n",
    "          config=cfg, name=f'encoderdecoderblock_{lyr}')(\n",
    "              x,\n",
    "              deterministic=deterministic,\n",
    "              decoder_mask=decoder_mask)\n",
    "    x = nn.LayerNorm(name='encoderdecoder_norm')(x)\n",
    "    logits_x = nn.Dense(\n",
    "        cfg.output_size,\n",
    "        kernel_init=cfg.kernel_init,\n",
    "        bias_init=cfg.bias_init,\n",
    "        name='logits_x')(x)\n",
    "    logits_dx = nn.Dense(\n",
    "        cfg.output_size,\n",
    "        kernel_init=cfg.kernel_init,\n",
    "        bias_init=cfg.bias_init,\n",
    "        name='logits_dx')(x)\n",
    "    return logits_x, logits_dx, None\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  \"\"\"Transformer pure decoder stack for language modelling.\n",
    "\n",
    "  Args:\n",
    "    config: TransformerConfig dataclass containing hyperparameters.\n",
    "  \"\"\"\n",
    "  config: TransformerConfig\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self,\n",
    "               inputs,\n",
    "               train):\n",
    "    \"\"\"Applies Transformer on the inputs.\n",
    "\n",
    "    N.b. does not support masking for incomplete sequences.\n",
    "\n",
    "    Args:\n",
    "      inputs: target data.\n",
    "\n",
    "    Returns:\n",
    "      logits array from transformer decoder.\n",
    "    \"\"\"\n",
    "    assert inputs['x'].ndim == 3  # (batch, len, channels)\n",
    "    assert inputs['y'].ndim == 3  # (batch, len, channels)\n",
    "    assert inputs['x'].shape[1] == inputs['y'].shape[1]\n",
    "\n",
    "    cfg = self.config\n",
    "    decoder_mask = None\n",
    "    if cfg.causal_x:\n",
    "      decoder_mask = nn.make_causal_mask(inputs['x'][...,0])\n",
    "    if cfg.physics_decoder:\n",
    "      logits_x, logits_dx, aux = PhysicsDecoder(config=cfg, name='decoder')(\n",
    "          inputs,\n",
    "          deterministic=not train,\n",
    "          decoder_mask=decoder_mask)\n",
    "    else:\n",
    "      logits_x, logits_dx, aux = Decoder(config=cfg, name='decoder')(\n",
    "          inputs,\n",
    "          deterministic=not train,\n",
    "          decoder_mask=decoder_mask)\n",
    "\n",
    "    return logits_x, logits_dx, aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ad91b36-43a0-452f-8dea-f07acfd92e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_l2(predictions, targets, padding):\n",
    "  \"\"\"Compute weighted cross entropy and entropy for log probs and targets.\n",
    "  Args:\n",
    "   predictions: [batch, length, dim] float array.\n",
    "   targets: categorical targets [batch, length, dim] float array.\n",
    "   padding: [batch, length] padding mask (1=padding)\n",
    "  Returns:\n",
    "    Tuple of scalar loss and batch normalizing factor.\n",
    "  \"\"\"\n",
    "  if predictions.ndim != targets.ndim :\n",
    "    raise ValueError('Incorrect shapes. Got shape %s predictions and %s targets' %\n",
    "                     (str(predictions.shape), str(targets.shape)))\n",
    "\n",
    "  predictions = predictions * (1 - padding[...,None])\n",
    "  targets = targets * (1 - padding[...,None])\n",
    "  loss = jnp.sum((predictions - targets) ** 2, axis=-1)  # Sum over channels\n",
    "  return loss.mean()   # Per-timestep average loss.\n",
    "\n",
    "\n",
    "def compute_hinge(values):\n",
    "  \"\"\"Compute hinge loss.\n",
    "  Args:\n",
    "   predictions: [batch, length] float array.\n",
    "  Returns:\n",
    "    Loss\n",
    "  \"\"\"\n",
    "  assert len(values.shape) == 2, f\"{len(values.shape)} != 2\"\n",
    "  loss = jnp.clip(values, a_min=0)\n",
    "  return loss.mean()   # Mean over time and batch.\n",
    "\n",
    "\n",
    "def compute_loss(p1, p2, aux, labels1, labels2, pad, alpha=0.0, aux_loss_weight=0.0):\n",
    "  l1 = compute_l2(p1, labels1, pad)\n",
    "  l2 = compute_l2(p2, labels2, pad)\n",
    "  l2_loss = (1 - alpha) * l1 + alpha * l2\n",
    "  aux_loss = 0.0\n",
    "  if aux is not None:\n",
    "    aux_loss = compute_hinge(aux)\n",
    "  else:\n",
    "    assert aux_loss_weight == 0.0\n",
    "  return l2_loss + aux_loss_weight * aux_loss, l2_loss, aux_loss\n",
    "\n",
    "\n",
    "def compute_metrics(p1, p2, aux, labels1, labels2, pad, alpha, aux_loss_weight):\n",
    "  \"\"\"Compute summary metrics.\"\"\"\n",
    "  loss, l2, aux = compute_loss(p1, p2, aux, labels1, labels2, pad, alpha, aux_loss_weight)\n",
    "  metrics ={\n",
    "      'loss': loss,\n",
    "      'l2_loss': l2,\n",
    "      'aux_loss': aux,\n",
    "  }\n",
    "  metrics = jax.lax.psum(metrics, axis_name=\"batch\")\n",
    "  return metrics\n",
    "\n",
    "\n",
    "def build_deltas(x):\n",
    "  dx = x[:, 1:, :] - x[:, :-1, :]\n",
    "  b, _, c = x.shape\n",
    "  first_dx = jnp.zeros(shape=(b, 1, c), dtype=jnp.float32)\n",
    "  dx = jnp.concatenate([first_dx, dx], axis=1)\n",
    "  return dx\n",
    "\n",
    "\n",
    "def train_step(state, inputs, model, learning_rate_fn, alpha, aux_loss_weight,\n",
    "               dropout_rng=None):\n",
    "  \"\"\"Perform a single training step.\"\"\"\n",
    "\n",
    "  dropout_rng = jax.random.fold_in(dropout_rng, state.step)\n",
    "  y = inputs['y']\n",
    "  dy = build_deltas(y)\n",
    "  pad = inputs['pad']\n",
    "\n",
    "  def loss_fn(params):\n",
    "    \"\"\"loss function used for training.\"\"\"\n",
    "    py, pdy, aux = model.apply(\n",
    "        {'params': params},\n",
    "        inputs,\n",
    "        train=True,\n",
    "        rngs={\"dropout\": dropout_rng})\n",
    "    loss = compute_loss(py, pdy, aux, y, dy, pad, alpha, aux_loss_weight)[0]\n",
    "    return loss, (py, pdy, aux)\n",
    "\n",
    "  step = state.step\n",
    "  lr = learning_rate_fn(step)\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  aux, grads = grad_fn(state.params)\n",
    "  _, (p1, p2, aux) = aux\n",
    "  grads = jax.lax.pmean(grads, 'batch')\n",
    "  new_state = state.apply_gradients(grads=grads)\n",
    "\n",
    "  metrics = compute_metrics(p1, p2, aux, y, dy, pad, alpha, aux_loss_weight)\n",
    "  metrics['learning_rate'] = lr\n",
    "\n",
    "  return new_state, metrics\n",
    "\n",
    "\n",
    "def eval_step(params, inputs, model, alpha, aux_loss_weight):\n",
    "  \"\"\"Calculate evaluation metrics on a inputs.\"\"\"\n",
    "  y = inputs['y']\n",
    "  dy = build_deltas(y)\n",
    "  pad = inputs['pad']\n",
    "  py, pdy, aux = model.apply({'params': params}, inputs, train=False)\n",
    "  return compute_metrics(py, pdy, aux, y, dy, pad, alpha, aux_loss_weight)\n",
    "\n",
    "\n",
    "# Call a jitted initialization function to get the initial parameter tree.\n",
    "@functools.partial(jax.jit, static_argnums=[0, 1])\n",
    "def initialize_variables(config, model, init_rng):\n",
    "  example = jnp.ones((1, config.max_len, 1), jnp.float32)\n",
    "  init_batch = {'x': example, 'y': example}\n",
    "  init_variables = model.init(init_rng, inputs=init_batch, train=False)\n",
    "  return init_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2321d464-b7de-4f02-9674-ebcb6156329a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721397227.606725     539 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-19 15:53:47.748179: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "hparams = {}\n",
    "\n",
    "# Setup\n",
    "hparams['model_dir'] = '/tmp/test'\n",
    "\n",
    "# Model params\n",
    "hparams['physics_decoder'] = False\n",
    "hparams['max_len'] = MAX_LEN\n",
    "hparams['num_layers'] = 4\n",
    "hparams['hidden_dim'] = 16\n",
    "hparams['mlp_dim'] = 4 * hparams['hidden_dim']\n",
    "hparams['num_heads'] = 2\n",
    "hparams['dropout_rate'] = 0.0\n",
    "hparams['attention_dropout_rate'] = 0.0\n",
    "hparams['alpha'] = 0.0\n",
    "hparams['aux_loss_weight'] = 0.0\n",
    "hparams['causal_x'] = True\n",
    "\n",
    "# Training params\n",
    "hparams['batch_size'] = 16\n",
    "hparams['learning_rate'] = 0.01\n",
    "hparams['num_train_steps'] = 1001\n",
    "hparams['eval_freq'] = 10\n",
    "hparams['weight_decay'] = 0.0\n",
    "hparams['random_seed'] = 0\n",
    "\n",
    "# Make sure tf does not allocate gpu memory.\n",
    "tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "\n",
    "if hparams['batch_size'] % jax.device_count() > 0:\n",
    "  raise ValueError('Batch size must be divisible by the number of devices')\n",
    "device_batch_size = hparams['batch_size'] // jax.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fdf9e32-e1b8-4d5a-b2d6-8bf2f9fbe742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0000,\ttrain loss 1.9271,\ttrain l2 1.9271,\ttrain aux 0.0000,\teval loss 1.8691,\teval l2 1.8691,\teval aux 0.0000,\tsteps/s 2.2\n",
      "Step: 0010,\ttrain loss 1.9020,\ttrain l2 1.9020,\ttrain aux 0.0000,\teval loss 1.8652,\teval l2 1.8652,\teval aux 0.0000,\tsteps/s 1.3\n",
      "Step: 0020,\ttrain loss 1.8746,\ttrain l2 1.8746,\ttrain aux 0.0000,\teval loss 1.8545,\teval l2 1.8545,\teval aux 0.0000,\tsteps/s 1.3\n",
      "Step: 0030,\ttrain loss 1.8794,\ttrain l2 1.8794,\ttrain aux 0.0000,\teval loss 1.8371,\teval l2 1.8371,\teval aux 0.0000,\tsteps/s 1.3\n",
      "Step: 0040,\ttrain loss 1.8492,\ttrain l2 1.8492,\ttrain aux 0.0000,\teval loss 1.8131,\teval l2 1.8131,\teval aux 0.0000,\tsteps/s 1.4\n",
      "Step: 0050,\ttrain loss 1.8486,\ttrain l2 1.8486,\ttrain aux 0.0000,\teval loss 1.7829,\teval l2 1.7829,\teval aux 0.0000,\tsteps/s 1.3\n",
      "Step: 0060,\ttrain loss 1.8020,\ttrain l2 1.8020,\ttrain aux 0.0000,\teval loss 1.7470,\teval l2 1.7470,\teval aux 0.0000,\tsteps/s 1.3\n",
      "Step: 0070,\ttrain loss 1.7336,\ttrain l2 1.7336,\ttrain aux 0.0000,\teval loss 1.7058,\teval l2 1.7058,\teval aux 0.0000,\tsteps/s 1.3\n",
      "Step: 0080,\ttrain loss 1.6897,\ttrain l2 1.6897,\ttrain aux 0.0000,\teval loss 1.6600,\teval l2 1.6600,\teval aux 0.0000,\tsteps/s 1.3\n",
      "Step: 0090,\ttrain loss 1.6767,\ttrain l2 1.6767,\ttrain aux 0.0000,\teval loss 1.6101,\teval l2 1.6101,\teval aux 0.0000,\tsteps/s 1.3\n",
      "Step: 0100,\ttrain loss 1.6086,\ttrain l2 1.6086,\ttrain aux 0.0000,\teval loss 1.5567,\teval l2 1.5567,\teval aux 0.0000,\tsteps/s 1.4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(hparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_train_steps\u001b[39m\u001b[38;5;124m'\u001b[39m]), train_iter):\n\u001b[1;32m     60\u001b[0m   batch \u001b[38;5;241m=\u001b[39m flax\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m.\u001b[39mcommon_utils\u001b[38;5;241m.\u001b[39mshard(\n\u001b[1;32m     61\u001b[0m       jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39m_numpy(), batch))  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m   state, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mp_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rngs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m   metrics_all\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m     66\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m hparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_freq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_ds, eval_ds = get_datasets_tf(batch_size=hparams['batch_size'])\n",
    "train_iter = iter(train_ds)\n",
    "\n",
    "config = TransformerConfig(\n",
    "    max_len=hparams['max_len'],\n",
    "    num_layers=hparams['num_layers'],\n",
    "    hidden_dim=hparams['hidden_dim'],\n",
    "    mlp_dim=hparams['mlp_dim'],\n",
    "    num_heads=hparams['num_heads'],\n",
    "    dropout_rate=hparams['dropout_rate'],\n",
    "    attention_dropout_rate=hparams['attention_dropout_rate'],\n",
    "    causal_x=hparams['causal_x'],\n",
    "    physics_decoder=hparams['physics_decoder'],\n",
    ")\n",
    "\n",
    "rng = jax.random.PRNGKey(hparams['random_seed'])\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "\n",
    "model = Transformer(config)\n",
    "init_variables = initialize_variables(config, model, init_rng)\n",
    "\n",
    "learning_rate_fn = create_learning_rate_scheduler(\n",
    "    base_learning_rate=hparams['learning_rate'])\n",
    "\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate_fn, b1=0.9, b2=0.98, eps=1e-9,\n",
    "    weight_decay=1e-1)\n",
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=init_variables[\"params\"],\n",
    "    tx=optimizer)\n",
    "state = flax.jax_utils.replicate(state)\n",
    "\n",
    "p_train_step = jax.pmap(\n",
    "    functools.partial(\n",
    "        train_step,\n",
    "        model=model,\n",
    "        learning_rate_fn=learning_rate_fn,\n",
    "        alpha=hparams['alpha'],\n",
    "        aux_loss_weight=hparams['aux_loss_weight']),\n",
    "    axis_name='batch',\n",
    "    donate_argnums=(0,))  # pytype: disable=wrong-arg-types\n",
    "p_eval_step = jax.pmap(\n",
    "    functools.partial(\n",
    "        eval_step,\n",
    "        model=model,\n",
    "        alpha=hparams['alpha'],\n",
    "        aux_loss_weight=hparams['aux_loss_weight']),\n",
    "    axis_name='batch')\n",
    "\n",
    "# We init the first set of dropout PRNG keys, but update it afterwards inside\n",
    "# the main pmap'd training update for performance.\n",
    "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
    "del rng\n",
    "\n",
    "metrics_all = []\n",
    "tick = time.time()\n",
    "best_dev_score = 1e6\n",
    "for step, batch in zip(range(hparams['num_train_steps']), train_iter):\n",
    "  batch = flax.training.common_utils.shard(\n",
    "      jax.tree.map(lambda x: x._numpy(), batch))  # pylint: disable=protected-access\n",
    "\n",
    "  state, metrics = p_train_step(state, batch, dropout_rng=dropout_rngs)\n",
    "  metrics_all.append(metrics)\n",
    "\n",
    "  if step % hparams['eval_freq'] == 0:\n",
    "    metrics_all = flax.training.common_utils.get_metrics(metrics_all)\n",
    "    lr = metrics_all.pop('learning_rate').mean()\n",
    "    summary = jax.tree.map(jnp.mean, metrics_all)\n",
    "    summary['learning_rate'] = lr\n",
    "    metrics_all = []\n",
    "\n",
    "    if jax.process_index() == 0:\n",
    "      tock = time.time()\n",
    "      steps_per_sec = hparams['eval_freq'] / (tock - tick)\n",
    "      tick = tock\n",
    "\n",
    "    eval_metrics = []\n",
    "    eval_iter = iter(eval_ds)\n",
    "    for i, eval_batch in enumerate(eval_iter):\n",
    "      eval_batch = jax.tree.map(lambda x: x._numpy(), eval_batch)  # pylint: disable=protected-access\n",
    "      # TODO: Handle final odd-sized batch by padding instead of dropping it.\n",
    "      cur_pred_batch_size = eval_batch['x'].shape[0]\n",
    "      if cur_pred_batch_size != hparams['batch_size']:\n",
    "        continue\n",
    "      eval_batch = flax.training.common_utils.shard(eval_batch)\n",
    "\n",
    "      metrics_from_eval = p_eval_step(state.params, eval_batch)\n",
    "      eval_metrics.append(metrics_from_eval)\n",
    "    eval_metrics = flax.training.common_utils.get_metrics(eval_metrics)\n",
    "    eval_summary = jax.tree.map(jnp.mean, eval_metrics)\n",
    "\n",
    "    print('Step: %04d,\\ttrain loss %0.4f,\\ttrain l2 %0.4f,\\ttrain aux %0.4f,\\teval loss %0.4f,\\teval l2 %0.4f,\\teval aux %0.4f,\\tsteps/s %0.1f' % (\n",
    "        step,\n",
    "        summary['loss'],\n",
    "        summary['l2_loss'],\n",
    "        summary['aux_loss'],\n",
    "        eval_summary['loss'],\n",
    "        eval_summary['l2_loss'],\n",
    "        eval_summary['aux_loss'],\n",
    "        steps_per_sec))\n",
    "\n",
    "    if best_dev_score > eval_summary['loss']:\n",
    "      best_dev_score = eval_summary['loss']\n",
    "      best_state = state\n",
    "\n",
    "    eval_summary['best_dev_score'] = best_dev_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f0b08-36a7-43f0-9d9d-739ae8e7b1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
